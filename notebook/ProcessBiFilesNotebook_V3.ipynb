{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ProcessBiFiles (Upload to Curated)\r\n",
        "\r\n",
        "This notebook will read incoming files from BI Layer, apply data quality checks and move data into the curated layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json # to parse JSON data\r\n",
        "import requests # python library for HTTP requests\r\n",
        "import logging # custom logging\r\n",
        "from datetime import datetime # library for manipulating dates and times.\r\n",
        "\r\n",
        "from pyspark.sql.functions import explode, arrays_zip, col, expr, when, md5, concat_ws # to process data in dataframes\r\n",
        "import pyspark.sql.functions as F # functions to perform operations on dataframes\r\n",
        "from pyspark.sql.types import * # all data types supported in PySpark SQL\r\n",
        "from notebookutils import mssparkutils # perform common tasks in synapse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set Notebook Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# storage account parameters\r\n",
        "storage_account = \"spfmvpsynapsedev00\"\r\n",
        "\r\n",
        "# conatiner parameters\r\n",
        "upload_container = \"spf-bi-upload\"\r\n",
        "metadata_container = \"spf-bi-metadata\"\r\n",
        "curated_container = \"spf-bi-curated\"\r\n",
        "\r\n",
        "# pipeline parameters\r\n",
        "pipeline_id = 'default_id'\r\n",
        "\r\n",
        "controlfile_name = \"control_table.parquet\"\r\n",
        "control_file_path = f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/{controlfile_name}\"\r\n",
        "\r\n",
        "adls_path = f\"abfss://{curated_container}@{storage_account}.dfs.core.windows.net\"\r\n",
        "\r\n",
        "# get list of files in the upload container\r\n",
        "list_of_files = mssparkutils.fs.ls(f\"abfss://{upload_container}@{storage_account}.dfs.core.windows.net/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Configure Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\r\n",
        "\r\n",
        "formatter = logging.Formatter(fmt = FORMAT)\r\n",
        "\r\n",
        "for handler in logging.getLogger().handlers:\r\n",
        "    handler.setFormatter(formatter)\r\n",
        "\r\n",
        "# Customize the log level for a specific logger\r\n",
        "syn_logger = logging.getLogger(pipeline_id)\r\n",
        "syn_logger.setLevel(logging.DEBUG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Use below functions for logging\r\n",
        "\r\n",
        "- syn_logger.debug(\"customized debug message\")\r\n",
        "- syn_logger.info(\"customized info message\")\r\n",
        "- syn_logger.warning(\"customized warning message\")\r\n",
        "- syn_logger.error(\"customized error message\")\r\n",
        "- syn_logger.critical(\"customized critical message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create control table parquet file if it does not exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "metadata_files = mssparkutils.fs.ls(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/\")\r\n",
        "file_exists = False\r\n",
        "\r\n",
        "controltable_schema = StructType([ \\\r\n",
        "StructField(\"raw_filepath\",StringType(),True), \\\r\n",
        "StructField(\"curated_filepath\",StringType(),True), \\\r\n",
        "StructField(\"processed\",BooleanType(),True), \\\r\n",
        "StructField(\"pipeline_id\",StringType(),True), \\\r\n",
        "StructField(\"raw_curated_timestamp\", TimestampType(), True), \\\r\n",
        "StructField(\"curated_rdv_timestamp\", TimestampType(), True), \\\r\n",
        "])\r\n",
        "\r\n",
        "for file in metadata_files:\r\n",
        "    if(file.name == 'control_table.parquet'):\r\n",
        "        # print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
        "        syn_logger.info(\"Control file exists. Do not create control table\")\r\n",
        "        file_exists = True\r\n",
        "\r\n",
        "if (file_exists == False):\r\n",
        "    syn_logger.info(\"COntrol file does not exist. Create empty control file with schema\")\r\n",
        "    new_df = spark.createDataFrame(data = [], schema = controltable_schema)\r\n",
        "    new_df.write.parquet(control_file_path)\r\n",
        "    syn_logger.warning(\"Created control table parquet\")\r\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get list of all control files in the upload container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_list_of_ctl_files():\r\n",
        "    list_of_ctl_files = []\r\n",
        "    for f in list_of_files:\r\n",
        "        if f.name.find('.ctl') != -1:\r\n",
        "            list_of_ctl_files.append(f.name)\r\n",
        "    return list_of_ctl_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "list_of_ctl_files = get_list_of_ctl_files()\r\n",
        "\r\n",
        "if(len(list_of_ctl_files) >=1):\r\n",
        "    syn_logger.info(\"{0} Files found in upload conatiner: {1}\".format(len(list_of_ctl_files), list_of_ctl_files))\r\n",
        "else:\r\n",
        "    syn_logger.warning(\"No files in Upload conatiner to process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Fetch Schema for control and data files from EDC\r\n",
        "\r\n",
        "#TODO\r\n",
        "- control file schema is still not present in EDC. So defining the control file schema in the notebook based on charset\r\n",
        "- Align with Luba/Aliaksei/Arindam to move this under EDC later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class EDC:\r\n",
        "    '''\r\n",
        "    Class for the EDC objects.\r\n",
        "    Attributes:\r\n",
        "       url: url of corresponding feeder system.\r\n",
        "       fs_id: name of the incoming feeder system.\r\n",
        "       edc_metadata_df: dataframe to store metadata from EDC.\r\n",
        "    '''\r\n",
        "    def __init__(self, fs_id):\r\n",
        "        self.fs_id = fs_id\r\n",
        "        self.url = None\r\n",
        "        self.edc_metadata_df = None\r\n",
        "        \r\n",
        "    def load_data_from_edc(self):\r\n",
        "        '''\r\n",
        "        This function reads metadata from the EDC endpoint and stores it as dataframe\r\n",
        "        '''\r\n",
        "        #TODO: based on fs_id fetch the corresponding EDC metadata\r\n",
        "        # get response from the url and read the json into a spark dataframe\r\n",
        "        syn_logger.info(\"Loading data from the EDC endpoint\")\r\n",
        "\r\n",
        "        self.url = \"https://spf-mvp-edc-connect.azurewebsites.net/api/fetch_metadata_from_edc?code=YpU8SZ6ftEsnB7g4qK46U1P_FQil0vXw5Eo2yLxFkuvHAzFuqJJjvg==\"+\"&fs_id=\"+self.fs_id\r\n",
        "        syn_logger.debug(\"Trying to reach the EDC endpoint url: {0}\".format(self.url))\r\n",
        "        \r\n",
        "        resp = requests.get(self.url)\r\n",
        "        \r\n",
        "        if(resp == 200):\r\n",
        "            syn_logger.debug(\"Internal server error while fetching schema from EDC endpoint: {0}\".format(resp))\r\n",
        "            return \"abort\"\r\n",
        "        else:\r\n",
        "            syn_logger.debug(\"Received a response from the EDC endpoint: {0}\".format(resp))\r\n",
        "\r\n",
        "        df = spark.read.json(sc.parallelize([resp.text]))\r\n",
        "\r\n",
        "        # Explode the JSON into facts and id\r\n",
        "        df = (df.withColumn('buffer', explode('items'))\r\n",
        "                .withColumn('facts', expr(\"buffer.facts\"))\r\n",
        "                .withColumn('id', expr(\"buffer.id\"))\r\n",
        "                .drop(*['items','metadata','buffer'])\r\n",
        "        )\r\n",
        "\r\n",
        "        # Flatten the json attributes into dataframe columns\r\n",
        "        df = df.withColumn(\"Position\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.Position')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"ColumnName\", expr(\"filter(facts, x -> x.attributeId = 'core.name')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"Charset\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.Code')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"FieldSeparator\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.FieldFormat')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"DataType\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.Datatype')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"Length\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.Length')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"Scale\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.Scale')\")[0][\"value\"])\\\r\n",
        "                                .withColumn(\"Nullable\", expr(\"filter(facts, x -> x.attributeId = 'com.infa.ldm.relational.Nullable')\")[0][\"value\"])\r\n",
        "\r\n",
        "        # sort the df by Position\r\n",
        "        df = df.orderBy(col('Position').cast(\"int\").asc())\r\n",
        "\r\n",
        "        # drop the facts and id columns now\r\n",
        "        df = df.drop(\"facts\", \"id\")\r\n",
        "\r\n",
        "        # convert datatypes to lower case\r\n",
        "        df = df.withColumn(\"DataType\", F.lower(F.col(\"DataType\")))\r\n",
        "\r\n",
        "        # set the edc_metadata_df attribute\r\n",
        "        self.edc_metadata_df = df\r\n",
        "\r\n",
        "        syn_logger.info(\"EDC data is sucessfully loaded into dataframe\")\r\n",
        "\r\n",
        "        return \"continue\"\r\n",
        "\r\n",
        "    def get_charset(self):\r\n",
        "        '''\r\n",
        "        Returns charset of the corresponding control file \r\n",
        "        '''\r\n",
        "        # get only the file level info from edc df\r\n",
        "        df = self.edc_metadata_df.filter(F.col('Position').isNull())\r\n",
        "        \r\n",
        "        # fetch charset from the dataframe\r\n",
        "        charset = df.select('Charset').collect()[0][0]\r\n",
        "\r\n",
        "        return charset\r\n",
        "\r\n",
        "    def get_field_separator(self):\r\n",
        "        '''\r\n",
        "        Returns the Field Separator to read the incoming .dat file\r\n",
        "        '''\r\n",
        "        # mapping to read Filed Separators to spark dataframe readable format\r\n",
        "        field_separator_mapping = {\r\n",
        "                                        \"TAB\": '\\t',\r\n",
        "                                        \"Space\": ' ',\r\n",
        "                                        \"Pipe\": '|',\r\n",
        "                                        \"Semi-Colon\": \";\"\r\n",
        "                                        }\r\n",
        "\r\n",
        "        # get only the file level info from edc df\r\n",
        "        df = self.edc_metadata_df.filter(F.col('Position').isNull())\r\n",
        "\r\n",
        "        # fetch the field separator from the dataframe\r\n",
        "        field_separator = df.select('FieldSeparator').collect()[0][0]\r\n",
        "\r\n",
        "        # map the fieldseparotr from edc to spark\r\n",
        "        field_separator = field_separator_mapping[field_separator]\r\n",
        "\r\n",
        "        return field_separator\r\n",
        "\r\n",
        "    def get_scehma_for_data_file(self):\r\n",
        "        '''\r\n",
        "        This function returns schema for data file\r\n",
        "        Arguemnts:\r\n",
        "            self\r\n",
        "        Return:\r\n",
        "            dat_schema: Schema to read data file\r\n",
        "        '''\r\n",
        "        # remove the undefined positions (eg: first row)\r\n",
        "        df = self.edc_metadata_df.filter(F.col('Position').isNotNull())\r\n",
        "        \r\n",
        "        dat_schema = StructType([])\r\n",
        "\r\n",
        "        for row in df.collect():\r\n",
        "            dat_schema.add(row['ColumnName'], StringType())\r\n",
        "\r\n",
        "        return dat_schema\r\n",
        "\r\n",
        "    def get_schema_for_control_file(self):\r\n",
        "        '''\r\n",
        "        This function returns the schema to read the control file based on the charset\r\n",
        "        Arguemnts:\r\n",
        "            self\r\n",
        "            charset: charset of the control file\r\n",
        "        Return:\r\n",
        "            schema: StructType() containing schema to read control file\r\n",
        "        '''\r\n",
        "        schema = None\r\n",
        "\r\n",
        "        char_set = self.get_charset()\r\n",
        "\r\n",
        "        if char_set == \"ASCII\" or char_set == \"EBCDIC\" or char_set == \"CSV_NAT\" or char_set == \"ISO 8859-2\":\r\n",
        "            schema = StructType([\r\n",
        "                StructField(\"fs_id\", StringType(), True),        \r\n",
        "                StructField(\"delivery\", StringType(), True),\r\n",
        "                StructField(\"delivery_number\", StringType(), True),\r\n",
        "                StructField(\"previous_delivery_number\", StringType(), True),        \r\n",
        "                StructField(\"sum_of_amount\", StringType(), True),        \r\n",
        "                StructField(\"number_of_records\", StringType(), True)\r\n",
        "            ])\r\n",
        "\r\n",
        "        elif char_set == \"CSV_INT\":\r\n",
        "            schema = StructType([\r\n",
        "                StructField(\"fs_id\", StringType(), True),\r\n",
        "                StructField(\"pfs_id\", StringType(), True),      \r\n",
        "                StructField(\"delivery\", StringType(), True),\r\n",
        "                StructField(\"delivery_number\", StringType(), True),\r\n",
        "                StructField(\"previous_delivery_number\", StringType(), True),        \r\n",
        "                StructField(\"sum_of_amount\", StringType(), True),        \r\n",
        "                StructField(\"number_of_records\", StringType(), True)\r\n",
        "            ])\r\n",
        "\r\n",
        "        return schema        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class Validator:\r\n",
        "    '''\r\n",
        "    Class for the Validator objects.\r\n",
        "\r\n",
        "    Attributes:\r\n",
        "  \r\n",
        "    '''\r\n",
        "    def __init__(self, data_df, edc_instance):\r\n",
        "        self.isValid = None\r\n",
        "        self.data_df = data_df\r\n",
        "        self.edc_df = edc_instance.edc_metadata_df\r\n",
        "\r\n",
        "    def number_of_columns_check(self):\r\n",
        "        '''\r\n",
        "        '''\r\n",
        "        number_of_cols_from_edc = self.edc_df.filter(F.col('Position').isNotNull()).count()\r\n",
        "        syn_logger.debug(\"Number of columns in data file {0}\".format(len(data_df.columns)))\r\n",
        "        syn_logger.debug(\"Number of columns in EDC {0}\".format(number_of_cols_from_edc))\r\n",
        "        \r\n",
        "        if(len(data_df.columns) == number_of_cols_from_edc):\r\n",
        "            self.isValid = True    \r\n",
        "        else:\r\n",
        "            self.isValid = False\r\n",
        "        \r\n",
        "        return self.isValid\r\n",
        "\r\n",
        "    def data_length_check(self):\r\n",
        "        '''\r\n",
        "        '''\r\n",
        "        rejected_cols = []\r\n",
        "        df = self.data_df\r\n",
        "\r\n",
        "        for column_name in self.data_df.columns:\r\n",
        "            # get actual legnt of each column from edc dataframe\r\n",
        "            actual_length = self.edc_df.filter(self.edc_df.ColumnName == column_name).select(\"Length\").collect()[0][0]\r\n",
        "            if actual_length:\r\n",
        "                df_filtered = df.withColumn(column_name, F.regexp_replace(column_name, ',', '')).filter( (F.length(F.col(column_name))  <= actual_length) | (F.col(column_name).isNull()) )\r\n",
        "                if(self.data_df.count() != df_filtered.count()):\r\n",
        "                    rejected_cols.append(column_name)\r\n",
        "        if len(rejected_cols) == 0:\r\n",
        "            self.isValid = True\r\n",
        "            syn_logger.info(\"length of the columns match with EDC specified length\")\r\n",
        "            \r\n",
        "        else:\r\n",
        "            self.isValid = False\r\n",
        "            syn_logger.critical(\"length of the columns do NOT match with EDC specified length for the following columns: {0}\".format(rejected_cols))\r\n",
        "\r\n",
        "        return self.isValid\r\n",
        "\r\n",
        "    def null_check(self):\r\n",
        "        '''\r\n",
        "        '''\r\n",
        "        nullable_colmuns = self.edc_df.filter(F.col('Nullable') == False).select('ColumnName')\r\n",
        "        syn_logger.info(\"Not Null columns from EDC: {0}\".format(nullable_colmuns.collect()))\r\n",
        "\r\n",
        "        if (nullable_colmuns):\r\n",
        "            for row in nullable_colmuns.collect():\r\n",
        "                if( self.data_df.filter(F.col(row['ColumnName']).isNull()).count() >= 1):\r\n",
        "                    self.isValid = False\r\n",
        "                    syn_logger.critical(\"Not Null constraint not met: {0}\".format(row['ColumnName']))            \r\n",
        "                else:\r\n",
        "                    self.isValid = True\r\n",
        "                    syn_logger.info(\"Not Null constraint met: {0}\".format(row['ColumnName']))\r\n",
        "                \r\n",
        "        return self.isValid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class Conversion:\r\n",
        "    '''\r\n",
        "    Class for the Conversion objects.\r\n",
        "\r\n",
        "    Attributes:\r\n",
        "  \r\n",
        "    '''\r\n",
        "    def __init__(self):\r\n",
        "        self.convert = None\r\n",
        "        self.data_df = data_df\r\n",
        "        self.edc_df = edc_instance.edc_metadata_df\r\n",
        "\r\n",
        "    def date_conversion(self):\r\n",
        "        '''\r\n",
        "        '''\r\n",
        "        date_colmuns = self.edc_df.filter(F.col('DataType') == 'date').select('ColumnName')\r\n",
        "        # TODO check len = 8\r\n",
        "        if (date_colmuns):\r\n",
        "            for row in date_colmuns.collect():\r\n",
        "                self.data_df = self.data_df.withColumn(row.ColumnName, when( (F.col(row.ColumnName).substr(5, 2).cast('int') <= 12) , F.to_date(col(row.ColumnName), \"yyyymmdd\") )\r\n",
        "                                    .when( (F.col(row.ColumnName).substr(3, 2).cast('int') <= 12), F.to_date(col(row.ColumnName), \"ddmmyyyy\"))\r\n",
        "                                    )\r\n",
        "                syn_logger.debug(\"date converion for the column: {0}\".format(row.ColumnName))\r\n",
        "\r\n",
        "        return self.data_df\r\n",
        "\r\n",
        "    def number_conversion(self):\r\n",
        "        '''\r\n",
        "        '''\r\n",
        "        number_dtype_colmuns = self.edc_df.filter(F.col('DataType') == 'number')\r\n",
        "        if (number_dtype_colmuns):\r\n",
        "            for row in number_dtype_colmuns.collect():\r\n",
        "                self.data_df = self.data_df.withColumn(row.ColumnName, F.regexp_replace(F.col(row.ColumnName), ',', '.').cast(DecimalType(precision = int(row.Length), scale = int(row.Scale))))\r\n",
        "                \r\n",
        "                syn_logger.debug(\"numeric converion for the column: {0}\".format(row.ColumnName))\r\n",
        "\r\n",
        "        return self.data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class ErrorHandler(Exception):\r\n",
        "    '''\r\n",
        "    Class for the Error Handling.\r\n",
        "\r\n",
        "    Attributes:\r\n",
        "        filename - filename which caused the error\r\n",
        "        message - explanation of the error\r\n",
        "    '''\r\n",
        "    def __init__(self, filename, *args):\r\n",
        "        self.filename = filename\r\n",
        "        if args:\r\n",
        "            self.message = args[0]\r\n",
        "        else:\r\n",
        "            self.message = None\r\n",
        "        super().__init__(self.message)\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        if self.message:\r\n",
        "            # print(\"{0}  - File - {1}\".format(syn_logger.error, self.filename))\r\n",
        "            return self.message\r\n",
        "\r\n",
        "    def move_file_to_failed_container(self):\r\n",
        "        '''\r\n",
        "        '''\r\n",
        "        return None\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# create a dict for instances of edc\r\n",
        "edc_instances_per_fs_id = {}\r\n",
        "\r\n",
        "# create one edc instance for one fs_id\r\n",
        "def create_edc_instances():\r\n",
        "    for ctl_file in list_of_ctl_files:\r\n",
        "        fs_id = ctl_file[:ctl_file.index('_')]\r\n",
        "        if fs_id.lower() not in edc_instances_per_fs_id:\r\n",
        "            edc_instances_per_fs_id[fs_id.lower()] = EDC(fs_id.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "map_2fs_family_df = spark.read.load(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/map_2fs_family.csv\", format='csv',header=True, delimiter=',', inferSchema=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "map_2mandt_df= spark.read.load(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/map_2mandt.csv\", format='csv',header=True, delimiter=',', inferSchema=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "create_edc_instances()\r\n",
        "result = []\r\n",
        "\r\n",
        "# read the ctl and data files and then validate the schema\r\n",
        "for ctl_file_name in list_of_ctl_files:\r\n",
        "    # get the corresponding edc instancce\r\n",
        "    fs_name = ctl_file_name[:ctl_file_name.index('_')]\r\n",
        "    syn_logger.info(\"Begin processing the file: {0}\".format(ctl_file_name))\r\n",
        "\r\n",
        "    edc_instance = edc_instances_per_fs_id[fs_name.lower()]\r\n",
        "    syn_logger.debug(\"Create edc instance for the file: {0}\".format(ctl_file_name))\r\n",
        "\r\n",
        "    # load edc metadata from the EDC REST end point\r\n",
        "    edc_resp = edc_instance.load_data_from_edc()\r\n",
        "    if(edc_resp == \"abort\"):\r\n",
        "        syn_logger.debug(\"Metdata not found in edc for: {0}\".format(ctl_file_name))\r\n",
        "        result.append({\"filename\": ctl_file_name, \"delivery_date\": delivery_date, \"field_separtor\": field_separator, \"is_valid\": False})\r\n",
        "        syn_logger.debug(\"Rejecting the file: {0}\".format(ctl_file_name))\r\n",
        "        break\r\n",
        "    else:\r\n",
        "        syn_logger.debug(\"Metdata succesfully loaded from edc for: {0}\".format(ctl_file_name))\r\n",
        "\r\n",
        "    # get Field Seperator\r\n",
        "    field_separator = edc_instance.get_field_separator()\r\n",
        "    syn_logger.debug(\"Field separator for the file: {0} is: {1}\".format(ctl_file_name, field_separator))\r\n",
        "    \r\n",
        "    # get ctl file schema\r\n",
        "    ctl_schema = edc_instance.get_schema_for_control_file()\r\n",
        "    syn_logger.debug(\"Control file schema loaded for: {0}\".format(ctl_file_name))\r\n",
        "    \r\n",
        "    # read control file\r\n",
        "    control_df = spark.read.load(f\"abfss://{upload_container}@{storage_account}.dfs.core.windows.net/{ctl_file_name}\", \r\n",
        "                                    format='csv',header=False, schema = ctl_schema, delimiter = field_separator)\r\n",
        "    \r\n",
        "    syn_logger.debug(\"Loaded the control file: {0}\".format(ctl_file_name))\r\n",
        "\r\n",
        "    feeder_system_id = control_df.collect()[0][0]\r\n",
        "    delivery_date = control_df.collect()[0][1]\r\n",
        "\r\n",
        "    syn_logger.debug(\"Feeder systme ID and Delivery Data: {0} is: {1}\".format(feeder_system_id, delivery_date))\r\n",
        "\r\n",
        "    # get shema for dat file\r\n",
        "    dat_schema = edc_instance.get_scehma_for_data_file()\r\n",
        "\r\n",
        "    # read data file\r\n",
        "    data_file_name = ctl_file_name.replace('.ctl', '.dat')\r\n",
        "    syn_logger.debug(\"Loaded data file schema for the file: {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    data_df = spark.read.load(f\"abfss://{upload_container}@{storage_account}.dfs.core.windows.net/{data_file_name}\", \r\n",
        "                                format = 'csv', header = False, schema = dat_schema, delimiter = field_separator )\r\n",
        "\r\n",
        "    syn_logger.debug(\"Laoded the data file: {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    # create an instance for validating the file strcuture\r\n",
        "    validator_instace = Validator(data_df, edc_instance)\r\n",
        "    syn_logger.debug(\"Create validator instance for the file: {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    syn_logger.debug(\"Technical validations begin: {0}\".format(data_file_name))\r\n",
        "    # perform technical validations\r\n",
        "    number_of_columns_check = validator_instace.number_of_columns_check()\r\n",
        "    if(number_of_columns_check):\r\n",
        "        syn_logger.debug(\"number of columns match: {0}\".format(data_file_name))\r\n",
        "    else:\r\n",
        "        syn_logger.critical(\"number of columns DO NOT match: {0}\".format(data_file_name))\r\n",
        "    \r\n",
        "    data_length_check = validator_instace.data_length_check()\r\n",
        "    if(data_length_check):\r\n",
        "        syn_logger.debug(\"lenght check satisfied: {0}\".format(data_file_name))\r\n",
        "    else:    \r\n",
        "        syn_logger.critical(\"length check failed: {0}\".format(data_file_name))\r\n",
        "    \r\n",
        "    null_check = validator_instace.null_check()\r\n",
        "    if(null_check):\r\n",
        "        syn_logger.debug(\"Null check success: {0}\".format(data_file_name))\r\n",
        "    else:    \r\n",
        "        syn_logger.critical(\"Null check failed: {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    # create a converiosn instance and performa necessary conversions\r\n",
        "\r\n",
        "    conversion_instace = Conversion()\r\n",
        "    data_df = conversion_instace.date_conversion()\r\n",
        "    syn_logger.debug(\"Date columns are converted: {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    data_df = conversion_instace.number_conversion()\r\n",
        "    syn_logger.debug(\"Numeric columns are converted: {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    fs_mandt_value = '#'\r\n",
        "    fs_mandt2_value = '#'\r\n",
        "\r\n",
        "    if 'fs_mandt' in data_df.columns:\r\n",
        "        fs_mandt_value = data_df.select(\"fs_mandt\").distinct().collect()[0][0]\r\n",
        "        \r\n",
        "    if 'fs_mandt2' in data_df.columns:\r\n",
        "        fs_mandt2_value = data_df.select(\"fs_mandt2\").distinct().collect()[0][0]\r\n",
        "    \r\n",
        "    syn_logger.debug(\"fs_mandt2_value: {0}\".format(fs_mandt2_value))\r\n",
        "\r\n",
        "    fs_family = None\r\n",
        "    fs_family_df = map_2fs_family_df.select('fs_family').filter(map_2fs_family_df.fs_id == feeder_system_id)\r\n",
        "    if fs_family_df.count() >= 1:\r\n",
        "        fs_family = fs_family_df.collect()[0][0]\r\n",
        "    \r\n",
        "    syn_logger.debug(\"fs_family: \".format(fs_family))\r\n",
        "\r\n",
        "    map_2mandt_df = map_2mandt_df.filter((map_2mandt_df.fs_family == fs_family))\r\n",
        "\r\n",
        "    mandt = None\r\n",
        "\r\n",
        "    #fs_mandt pfs_id fs_mandt2\r\n",
        "    valuesToCompare = [fs_mandt2_value, feeder_system_id, fs_mandt_value]\r\n",
        "\r\n",
        "    for i in range(len(valuesToCompare)):\r\n",
        "\r\n",
        "        mandt_df = map_2mandt_df.select('mandt').filter((map_2mandt_df.fs_mandt2 == valuesToCompare[0]) & (map_2mandt_df.pfs_id == valuesToCompare[1]) & (map_2mandt_df.fs_mandt == valuesToCompare[2]))\r\n",
        "        if mandt_df.count() >= 1:\r\n",
        "            syn_logger.debug(\"found mandt {}\".format(mandt_df.collect()[0][0]))\r\n",
        "            mandt = mandt_df.collect()[0][0]\r\n",
        "            break\r\n",
        "        else:\r\n",
        "            valuesToCompare[i] = '#'\r\n",
        "\r\n",
        "    # add business Id column. This is temporary solution until we know about the Business Id In RDV\r\n",
        "    col_list=[]\r\n",
        "    for i in data_df.columns:\r\n",
        "        col_list.append(i)\r\n",
        "\r\n",
        "    data_df = data_df.withColumn(\"financial_transaction_bus_id\", md5(concat_ws(\"\", *col_list)))\r\n",
        "\r\n",
        "    syn_logger.debug(\"Added Business Id column {0}\".format(data_file_name))\r\n",
        "\r\n",
        "    dt = datetime.strptime(delivery_date, '%Y%m%d')\r\n",
        "\r\n",
        "    syn_logger.debug(\"Check if the data file passed all technical validations{0}\".format(data_file_name))\r\n",
        "\r\n",
        "    if(validator_instace.isValid):\r\n",
        "        syn_logger.debug(\"File {0} passed all the technical validations\".format(data_file_name))\r\n",
        "        syn_logger.debug(\"Moving file {0} to curated container\".format(data_file_name))\r\n",
        "\r\n",
        "        parquet_file_name = data_file_name.replace('.dat', '.parquet')\r\n",
        "        relative_file_path = '/' + str(mandt) + '/' + fs_name +'/' + str(dt.year) + '/' + str(dt.month) + '/' + str(dt.day) +  '/' + parquet_file_name\r\n",
        "        file_path = adls_path + relative_file_path\r\n",
        "        data_df.write.mode(\"overwrite\").parquet(file_path)\r\n",
        "        \r\n",
        "        syn_logger.debug(\"Parquet file succesfully written {0}\".format(file_path))\r\n",
        "    else:\r\n",
        "        syn_logger.debug(\"File {0} did NOT pass all the technical validations\".format(data_file_name))\r\n",
        "        syn_logger.debug(\"Move file {0} to reject container\".format(data_file_name))\r\n",
        "        syn_logger.debug(\"DO NOT write Parquet file to curated container {0}\")\r\n",
        "\r\n",
        "    '''\r\n",
        "    append the control file with files that are being moved to curated container\r\n",
        "    '''\r\n",
        "\r\n",
        "    info = [ctl_file_name, relative_file_path, False, pipeline_id, datetime.now(), datetime.now() ]\r\n",
        "    \r\n",
        "    # load control table parquet file\r\n",
        "    # controltable_df = spark.read.load('abfss://spf-bi-metadata@{storage_account}.dfs.core.windows.net/control_table.parquet', format='parquet', schema = controltable_schema)\r\n",
        "    controltable_df = spark.read.load(control_file_path, format='parquet', schema = controltable_schema)\r\n",
        "    syn_logger.debug(\"Control file loaded\")\r\n",
        "\r\n",
        "    if(controltable_df.filter(controltable_df.raw_filepath == ctl_file_name).count() == 0):\r\n",
        "        syn_logger.info(\"No file exists in in control table. therfore, appending the new control file info {0}\".format(ctl_file_name))\r\n",
        "        temp_df = spark.createDataFrame(data = [info], schema = controltable_schema)\r\n",
        "        controltable_df = controltable_df.union(temp_df)\r\n",
        "        controltable_df.write.parquet(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/control_table_new.parquet\")\r\n",
        "        mssparkutils.fs.rm(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/control_table.parquet\", True)\r\n",
        "        controltable_df_temp = spark.read.load(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/control_table_new.parquet\", format='parquet', schema = controltable_schema)\r\n",
        "        controltable_df_temp.write.parquet(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/control_table.parquet\")\r\n",
        "        mssparkutils.fs.rm(f\"abfss://{metadata_container}@{storage_account}.dfs.core.windows.net/control_table_new.parquet\", True)\r\n",
        "        syn_logger.debug(\"Updated the control file with file: {0}\".format(ctl_file_name))\r\n",
        "    else:\r\n",
        "        syn_logger.debug(\"Filename is not appended to control file. File has already been processed {0}\".format(ctl_file_name))\r\n",
        "\r\n",
        "    result.append({\"filename\": ctl_file_name, \"delivery_date\": delivery_date, \"field_separtor\": field_separator, \"is_valid\": validator_instace.isValid})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils\r\n",
        "mssparkutils.notebook.exit(result)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}